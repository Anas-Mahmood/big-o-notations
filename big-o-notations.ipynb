{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Big-O Notation**","metadata":{}},{"cell_type":"markdown","source":"**The Big-O notation is at its heart a mathematical notation, used to compare the rate of convergence of functions.\nLet n -> f(n) and n -> g(n) be functions defined over the natural numbers. Then we say that f = O(g) if and\nonly if f(n)/g(n) is bounded when n approaches infinity. In other words, f = O(g) if and only if there exists a\nconstant A, such that for all n, f(n)/g(n) <= A.\nActually the scope of the Big-O notation is a bit wider in mathematics but for simplicity I have narrowed it to what is\nused in algorithm complexity analysis : functions defined on the naturals, that have non-zero values, and the case\nof n growing to infinity**","metadata":{}},{"cell_type":"markdown","source":"..............................................................................................................................................................................................................................................................................................................................................................................................................................","metadata":{}},{"cell_type":"markdown","source":"# **How is it used ?**","metadata":{}},{"cell_type":"markdown","source":"**When comparing algorithm performance, we are interested in the number of operations that an algorithm\nperforms. This is called time complexity. In this model, we consider that each basic operation (addition,\nmultiplication, comparison, assignment, etc.) takes a fixed amount of time, and we count the number of such\noperations. We can usually express this number as a function of the size of the input, which we call n. And sadly,\nthis number usually grows to infinity with n (if it doesn't, we say that the algorithm is O(1)). We separate our\nalgorithms in big speed classes defined by Big-O : when we speak about a \"O(n^2) algorithm\", we mean that the\nnumber of operations it performs, expressed as a function of n, is a O(n^2). This says that our algorithm is\napproximately as fast as an algorithm that would do a number of operations equal to the square of the size of its\ninput, or faster. The \"or faster\" part is there because I used Big-O instead of Big-Theta, but usually people will say\nBig-O to mean Big-Theta.\nGoalKicker.com â€“ Algorithms Notes for Professionals 9\nWhen counting operations, we usually consider the worst case: for instance if we have a loop that can run at most n\ntimes and that contains 5 operations, the number of operations we count is 5n. It is also possible to consider the\naverage case complexity**","metadata":{}},{"cell_type":"markdown","source":"..............................................................................................................................................................................................................................................................................................................................................................................................................................","metadata":{}},{"cell_type":"markdown","source":"# **Quick note**","metadata":{}},{"cell_type":"markdown","source":"**A fast algorithm is one that performs few operations, so if the number of operations grows to infinity\nfaster, then the algorithm is slower: O(n) is better than O(n^2).\nWe are also sometimes interested in the space complexity of our algorithm. For this we consider the number of\nbytes in memory occupied by the algorithm as a function of the size of the input, and use Big-O the same way**","metadata":{}},{"cell_type":"markdown","source":"..............................................................................................................................................................................................................................................................................................................................................................................................................................","metadata":{}},{"cell_type":"markdown","source":"# **A Simple Loop**","metadata":{}},{"cell_type":"code","source":"# The following function finds the maximal element in an array:\nint find_max(const int *array, size_t len) {\n int max = INT_MIN;\n for (size_t i = 0; i < len; i++) {\n if (max < array[i]) {\n max = array[i];\n }\n }\n return max;\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Understand the Code**","metadata":{}},{"cell_type":"markdown","source":"**The input size is the size of the array, which I called len in the code.\nLet's count the operations.\nint max = INT_MIN;\nsize_t i = 0;\nThese two assignments are done only once, so that's 2 operations. The operations that are looped are:\nif (max < array[i])\ni++;\nmax = array[i]\nSince there are 3 operations in the loop, and the loop is done n times, we add 3n to our already existing 2\noperations to get 3n + 2. So our function takes 3n + 2 operations to find the max (its complexity is 3n + 2). This is\na polynomial where the fastest growing term is a factor of n, so it is O(n)**","metadata":{}},{"cell_type":"markdown","source":"..............................................................................................................................................................................................................................................................................................................................................................................................................................","metadata":{}},{"cell_type":"markdown","source":"# **A Nested Loop**","metadata":{}},{"cell_type":"code","source":"# The following function checks if an array has any duplicates by taking each element, then iterating over the whole\n# array to see if the element is there\n\n_Bool contains_duplicates(const int *array, size_t len) {\n for (int i = 0; i < len - 1; i++) {\n for (int j = 0; j < len; j++) {\n if (i != j && array[i] == array[j]) {\n return 1;\n }\n }\n }\n return 0;\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Understand the Code**","metadata":{}},{"cell_type":"markdown","source":"**The inner loop performs at each iteration a number of operations that is constant with n. The outer loop also does\na few constant operations, and runs the inner loop n times. The outer loop itself is run n times. So the operations\ninside the inner loop are run n^2 times, the operations in the outer loop are run n times, and the assignment to i is\ndone one time. Thus, the complexity will be something like an^2 + bn + c, and since the highest term is n^2, the O\nnotation is O(n^2)**","metadata":{}},{"cell_type":"markdown","source":"# **Build Your Logic**","metadata":{}},{"cell_type":"code","source":"# As you may have noticed, we can improve the algorithm by avoiding doing the same comparisons multiple times.\n# We can start from i + 1 in the inner loop, because all elements before it will already have been checked against all\n# array elements, including the one at index i + 1. This allows us to drop the i == j check.\n\n_Bool faster_contains_duplicates(const int *array, size_t len) {\n for (int i = 0; i < len - 1; i++) {\n for (int j = i + 1; j < len; j++) {\n if (array[i] == array[j]) {\n return 1;\n }\n }\n }\n return 0;\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obviously, this second version does less operations and so is more efficient. How does that translate to Big-O\nnotation? Well, now the inner loop body is run 1 + 2 + ... + n - 1 = n(n-1)/2 times. This is still a polynomial of\nthe second degree, and so is still only O(n^2). We have clearly lowered the complexity, since we roughly divided by\n2 the number of operations that we are doing, but we are still in the same complexity class as defined by Big-O. In\norder to lower the complexity to a lower class we would need to divide the number of operations by something that\ntends to infinity with n.**","metadata":{}},{"cell_type":"markdown","source":"..............................................................................................................................................................................................................................................................................................................................................................................................................................","metadata":{}},{"cell_type":"markdown","source":"# **O(log n) types of Algorithms**","metadata":{}},{"cell_type":"markdown","source":"**Let's say we have a problem of size n. Now for each step of our algorithm(which we need write), our original\nproblem becomes half of its previous size(n/2).\nSo at each step, our problem becomes half.**\n\n**Step Problem**\n1. n/2\n2. n/4\n3. n/8\n4. n/16\n\n\n**When the problem space is reduced(i.e solved completely), it cannot be reduced any further(n becomes equal to 1)\nafter exiting check condition.**\n\n1. Let's say at kth step or number of operations:\nproblem-size = 1\n2. But we know at kth step, our problem-size should be:\nproblem-size = n/2k\n3. From 1 and 2:\nn/2k = 1 or\nn = 2k\n4. Take log on both sides\nloge n = k loge2\nor\nk = loge n / loge 2\n5. Using formula logx m / logx n = logn m\nk = log2 n\nor simply k = log n\n**Now we know that our algorithm can run maximum up to log n, hence time complexity comes as\nO( log n)**","metadata":{}},{"cell_type":"markdown","source":"# **Binary Search Algorithm**","metadata":{}},{"cell_type":"code","source":"int bSearch(int arr[],int size,int item){\nint low=0;\nint high=size-1;\nwhile(low<=high){ \n mid=low+(high-low)/2; \n if(arr[mid]==item) \n return mid; \n else if(arr[mid]<item) \n low=mid+1; \n else high=mid-1; \n } \n return â€“1;// Unsuccessful result\n}\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"..............................................................................................................................................................................................................................................................................................................................................................................................................................","metadata":{}},{"cell_type":"markdown","source":"# **An O(log n) example**","metadata":{}},{"cell_type":"markdown","source":"# **Build Your Logic**","metadata":{}},{"cell_type":"markdown","source":"**Consider the following problem:\nL is a sorted list containing n signed integers (n being big enough), for example [-5, -2, -1, 0, 1, 2, 4] (here, n\nhas a value of 7). If L is known to contain the integer 0, how can you find the index of 0 ?\nNaÃ¯ve approach\nThe first thing that comes to mind is to just read every index until 0 is found. In the worst case, the number of\noperations is n, so the complexity is O(n).\nThis works fine for small values of n, but is there a more efficient way ?**","metadata":{}},{"cell_type":"markdown","source":"# **Code**","metadata":{}},{"cell_type":"code","source":"a = 0\nb = n-1\nwhile True:\n h = (a+b)//2 ## // is the integer division, so h is an integer\n if L[h] == 0:\n return h\n elif L[h] > 0:\n b = h\n elif L[h] < 0:\n a = h\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Explanation**","metadata":{}},{"cell_type":"markdown","source":"**a and b are the indexes between which 0 is to be found. Each time we enter the loop, we use an index between a\nand b and use it to narrow the area to be searched.\nIn the worst case, we have to wait until a and b are equal. But how many operations does that take? Not n, because\neach time we enter the loop, we divide the distance between a and b by about two. Rather, the complexity is O(log\nn).\nExplanation\nNote: When we write \"log\", we mean the binary logarithm, or log base 2 (which we will write \"log_2\"). As O(log_2 n) = O(log\nn) (you can do the math) we will use \"log\" instead of \"log_2\".\nGoalKicker.com â€“ Algorithms Notes for Professionals 13\nLet's call x the number of operations: we know that 1 = n / (2^x).\nSo 2^x = n, then x = log n\nConclusion\nWhen faced with successive divisions (be it by two or by any number), remember that the complexity is logarithmic**","metadata":{}},{"cell_type":"markdown","source":"..............................................................................................................................................................................................................................................................................................................................................................................................................................","metadata":{}},{"cell_type":"markdown","source":"***If you like this, please upvote and share this notebook***","metadata":{}}]}